---
---

@article{Bhanja2018,
  author    = {Samit Bhanja and
               Abhishek Das},
  title     = {Impact of Data Normalization on Deep Neural Network for Time Series
               Forecasting},
  journal   = {CoRR},
  volume    = {abs/1812.05519},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.05519},
  archivePrefix = {arXiv},
  eprint    = {1812.05519},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-05519.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{Brownlee2018,
author = {Brownlee, Jason},
doi = {10.1007/978-3-030-64777-3_9},
file = {:Users/ronaldogamermann/Library/Mobile Documents/com$\sim$apple$\sim$CloudDocs/dokumen.pub_deep-learning-for-time-series-forecasting-predict-the-future-with-mlps-cnns-and-lstms-in-python.pdf:pdf;:Users/ronaldogamermann/Library/Application Support/Mendeley Desktop/Downloaded/Brownlee_2018.pdf:pdf},
mendeley-groups = {Machine Learning/Deep Learing},
pages = {107--131},
publisher = {brownleee},
title = {{Deep Learning for Time Series Forecasting: Predict the Future with MLPs, CNNs and LSTMs in Python}},
year = {2018}
}




@article{Krizhevsky2012,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
journal = {Advances in Neural Information Processing Systems 25},
mendeley-groups = {Machine Learning/Deep Learing/CNN},
pages = {1097--1105},
title = {{ImageNet Classification with Deep Convolutional Neural Neural Networks}},
year = {2012}
}


@article{Yann1995,
abstract = {The ability of multilayer back-propagation networks to learn complex, high-dimensional, nonlinear mappings from large collections of examples makes them obvious candidates for image recognition or speech recognition tasks (see PATTERN RECOGNITION AND NEURAL NETWORKS). In the traditional model of pattern recognition, a hand-designed feature extractor gathers relevant information from the input and eliminates irrelevant variabilities. A trainable classifier then categorizes the resulting feature vectors (or strings of symbols) into classes. In this scheme, standard, fully-connected multilayer networks can be used as classifiers. A potentially more interesting scheme is to eliminate the feature extractor, feeding the network with "raw" inputs (e.g. normalized images), and to rely on backpropagation to turn the first few layers into an appropriate feature extractor. While this can be done with an ordinary fully connected feed-forward network with some success for tasks such as character recognition, there are problems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Yann, LeCun and Yoshua, Bengio},
eprint = {arXiv:1011.1669v3},
isbn = {0-7803-8359-1},
issn = {1098-7576},
journal = {The handbook of brain theory and neural networks},
mendeley-groups = {Machine Learning/Deep Learing/CNN},
pages = {255--258},
pmid = {17001990},
title = {{Convolutional Networks for Images, Speech, and Time-Series}},
url = {https://www.researchgate.net/profile/Yann_Lecun/publication/2453996_Convolutional_Networks_for_Images_Speech_and_Time-Series/links/0deec519dfa2325502000000.pdf%0Ahttps://www.researchgate.net/profile/Yann_Lecun/publication/2453996_Convolutional_Networks_fo},
year = {1995}
}


@article{IsmailFawaz2019,
abstract = {Time Series Classification (TSC) is an important and challenging problem in data mining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such as text and audio can also be processed with DNNs to reach state-of-the-art performance for document classification and speech recognition. In this article, we study the current state-of-the-art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By training 8730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.},
archivePrefix = {arXiv},
arxivId = {1809.04356},
author = {{Ismail Fawaz}, Hassan and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre Alain},
doi = {10.1007/s10618-019-00619-1},
eprint = {1809.04356},
issn = {1573756X},
journal = {Data Mining and Knowledge Discovery},
keywords = {Classification,Deep learning,Review,Time series},
mendeley-groups = {Machine Learning/Deep Learing/CNN},
number = {4},
pages = {917--963},
title = {{Deep learning for time series classification: a review}},
volume = {33},
year = {2019}
}


@article{Albawi2017,
abstract = {The term Deep Learning or Deep Neural Network refers to Artificial Neural Networks (ANN) with multi layers . Over the last few decades, it has been considered to be one of the most powerful tools, and has become very popular in the literature as it is able to handle a huge amount of data. The interest in having deeper hidden layers has recently begun to surpass classical methods performance in different fields; especially in pattern recognition. One of the most popular deep neural networks is the Convolutional Neural Network (CNN). It take this name from mathematical linear operation between matrixes called convolution. CNN have multiple layers; including convolutional layer, non-linearity layer, pooling layer and fullyconnected layer. The convolutional and fully- connected layers have parameters but pooling and non-linearity layers don't have parameters. The CNN has an excellent performance in machine learning problems. Specially the applications that deal with image data, such as largest image classification data set (Image Net), computer vision, and in natural language processing (NLP) and the results achieved were very amazing . In this paper we will explain and define all the elements and important issues related to CNN, and how these elements work. In addition, we will also state the parameters that effect CNN efficiency. This paper assumes that the readers have adequate knowledge about both machine learning and artificial neural network.},
author = {Albawi, Saad and Mohammed, Tareq Abed Mohammed and Alzawi, Saad},
isbn = {9781538619490},
journal = {Ieee},
keywords = {artificial neural networks,computer vision,convolutional neural networks,deep,image,learning,machine learning},
mendeley-groups = {Machine Learning/Deep Learing/CNN},
title = {{Layers of a Convolutional Neural Network}},
year = {2017}
}

